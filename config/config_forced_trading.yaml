# Configuration for Forced Trading Training
# Designed to overcome ultra-conservative behavior

# Data Configuration
data_source: "alpaca"
data_days: 60
cache_data: true
use_real_data: true

# Trading Symbols - Liquid ETFs for consistent data
symbols:
  - SPY   # S&P 500 - Most liquid
  - QQQ   # NASDAQ - High volume

# Training Parameters - Optimized for Forced Trading
num_episodes: 3000
episode_length: 150     # Shorter episodes for faster learning
batch_size: 256         # Large batches for stable learning
n_epochs: 20           # More training per batch

# Aggressive Learning Parameters
learning_rate: 2e-3        # High learning rate
clstm_learning_rate: 5e-3  # Very high CLSTM LR
gamma: 0.9                 # Lower gamma for immediate rewards
clip_epsilon: 0.4          # High exploration
value_coef: 2.0           # Strong value learning
entropy_coef: 0.1         # Very high exploration
max_grad_norm: 2.0        # Allow large gradients

# Forced Trading Parameters
min_trades_per_episode: 3    # Minimum trades required
max_hold_steps: 40          # Force action every 40 steps
trading_bonus: 0.02         # Bonus for trading
inaction_penalty: 0.015     # Penalty for not trading
force_trading: true         # Enable forced trading

# Curriculum Learning
use_curriculum: true
curriculum_stages: 3
stage_1_episodes: 1000      # Heavy forced trading
stage_2_episodes: 1000      # Moderate forced trading  
stage_3_episodes: 1000      # Minimal forced trading

# Risk Parameters (Relaxed for Learning)
initial_capital: 100000
max_positions: 2            # Start with fewer positions
position_size_pct: 0.08    # 8% per position
risk_free_rate: 0.05
transaction_cost: 0.0005    # Low costs for learning

# Reward Engineering (Aggressive)
reward_scaling: 20.0        # High reward scaling
profit_bonus: 5.0          # Large bonus for profits
loss_penalty: 1.0          # Moderate loss penalty
trade_frequency_bonus: 0.2  # Strong trading bonus
risk_penalty: 0.3          # Moderate risk penalty

# Data Quality (Very Relaxed)
min_data_quality: 0.1      # Very low threshold
min_volume_threshold: 100   # Very low volume requirement
max_spread_threshold: 0.1   # 10% max spread

# Experience Replay (Enhanced)
buffer_size: 100000        # Very large buffer
min_buffer_size: 500       # Start training early
replay_ratio: 0.4          # 40% replay vs new experience

# Exploration Schedule
warmup_episodes: 50        # Short warmup
exploration_decay: 0.999   # Very slow decay
min_exploration: 0.2       # High minimum exploration

# Pre-training (Aggressive)
pretrain_epochs: 200       # Extensive pre-training
pretrain_lr: 1e-2         # Very high pre-training LR
use_supervised_pretraining: true
supervised_data_ratio: 0.5

# Regularization (Light)
dropout_rate: 0.05         # Very light dropout
weight_decay: 1e-6         # Very light weight decay
gradient_noise: 0.02       # More noise for exploration

# GPU Configuration
use_multi_gpu: true
gpu_memory_fraction: 0.9
mixed_precision: false

# Monitoring (Frequent)
save_frequency: 25         # Save very frequently
eval_frequency: 10         # Evaluate very often
log_frequency: 5           # Log every 5 episodes
checkpoint_dir: "checkpoints/forced_trading"
auto_resume: true

# Debugging
debug_mode: true
save_episode_data: true
analyze_actions: true
track_gradients: true

# Logging
log_wandb: true
wandb_project: "forced-trading-options"
wandb_tags: ["forced", "curriculum", "aggressive"]

# Environment Settings
render_mode: false
seed: 42
deterministic: false

# Action Space (Modified for Trading)
action_space_type: "discrete"
num_discrete_actions: 31
disable_hold_action: false    # We'll penalize it instead

# Observation Space
include_technical_indicators: true
include_market_microstructure: true
include_sentiment_data: false
lookback_window: 15           # Shorter lookback

# Risk Management (Relaxed)
max_drawdown: 0.3            # 30% max drawdown
stop_loss: 0.08             # 8% stop loss
take_profit: 0.2            # 20% take profit
max_leverage: 1.5           # 1.5x max leverage

# Market Hours
market_open: "09:30"
market_close: "16:00"
timezone: "US/Eastern"

# Data Validation (Relaxed)
validate_data: true
fix_missing_data: true
interpolate_gaps: true
remove_outliers: false      # Keep all data for learning
outlier_threshold: 5.0

# Special Forced Trading Settings
force_trade_probability: 0.3    # 30% chance to force trade
min_episode_trades: 3           # Minimum trades per episode
max_consecutive_holds: 30       # Max holds before forcing trade
trading_warmup_episodes: 100    # Episodes to force heavy trading

# Reward Shaping for Trading
no_trade_penalty: -0.01        # Penalty for no trades
trade_completion_bonus: 0.005   # Bonus for completing trades
position_holding_penalty: -0.002 # Penalty for holding too long
profit_momentum_bonus: 0.01     # Bonus for consecutive profits

# Learning Rate Schedule
lr_schedule: "cosine"           # Cosine annealing
lr_warmup_episodes: 100        # LR warmup period
lr_min_factor: 0.1             # Minimum LR factor

# Advanced Features
use_attention: true            # Use attention in model
use_residual_connections: true # Residual connections
use_layer_norm: true          # Layer normalization
use_spectral_norm: false      # No spectral norm (too restrictive)
