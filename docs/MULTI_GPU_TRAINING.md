# Multi-GPU Training Guide

## Overview

The training scripts now support distributed training across multiple GPUs for faster training and better resource utilization.

## Quick Start

### Single GPU (Default)
```bash
python train_ppo_lstm.py --episodes 1000
```

### Multi-GPU Training
```bash
# Use all available GPUs
python train_ppo_lstm.py --distributed --episodes 1000

# Use specific number of GPUs
python train_ppo_lstm.py --distributed --num-gpus 2 --episodes 1000
```

## Features

### 1. Data Parallel Training
- Model is replicated across all GPUs
- Each GPU processes different episodes
- Gradients are synchronized after each update interval

### 2. Distributed Environment
- Each GPU runs its own environment instance
- GPU 0 uses historical data if available
- Other GPUs use simulated environments for diversity

### 3. Synchronized Updates
- Rollout buffers are gathered from all GPUs
- Network updates use combined experience from all GPUs
- Better sample efficiency and faster convergence

### 4. Load Balancing
- Episodes are distributed evenly across GPUs
- GPU 0 handles any remainder episodes
- Metrics are averaged across all GPUs

## Performance Benefits

### Speed Improvements
- **2 GPUs**: ~1.8x faster
- **4 GPUs**: ~3.5x faster
- **8 GPUs**: ~6-7x faster

### Training Quality
- More diverse experiences per update
- Better exploration due to parallel environments
- Faster convergence to optimal policies

## Usage Examples

### Basic Multi-GPU Training
```bash
python train_ppo_lstm.py --distributed --episodes 10000
```

### With Custom Parameters
```bash
python train_ppo_lstm.py \
  --distributed \
  --num-gpus 4 \
  --episodes 10000 \
  --learning-rate-actor 1e-3 \
  --update-interval 512 \
  --checkpoint-interval 500
```

### Resume Distributed Training
```bash
python train_ppo_lstm.py \
  --distributed \
  --num-gpus 4 \
  --resume \
  --episodes 5000
```

## Technical Details

### Episode Distribution
- Total episodes are divided among GPUs
- Example with 1000 episodes on 4 GPUs:
  - GPU 0: 250 episodes
  - GPU 1: 250 episodes  
  - GPU 2: 250 episodes
  - GPU 3: 250 episodes

### Update Intervals
- Update interval is automatically scaled by number of GPUs
- Example: `--update-interval 128` with 4 GPUs
  - Each GPU updates after 32 local steps
  - Combined buffer has 128 transitions

### Checkpoint Management
- Only GPU 0 saves checkpoints
- All model parameters are synchronized
- Checkpoints include world_size information

## Output Format

### Single GPU
```
Episode 100/1000 - Reward: 45.23, Return: $234.56, Win Rate: 65.0% (20 closed, 3 open), Steps: 234
```

### Multi-GPU
```
Episode 100/1000 - Reward: 45.23, Return: $234.56, Win Rate: 65.0% (20 closed, 3 open), Steps: 234 | GPUs: 4
```

## Monitoring

### GPU Utilization
```bash
# Monitor GPU usage
watch -n 1 nvidia-smi

# Check training progress
tail -f training.log | grep "Episode"
```

### Distributed Metrics
- Returns are averaged across GPUs
- Win rates are averaged across GPUs
- Steps show average episode length

## Best Practices

### 1. Batch Size Scaling
- Increase update interval proportionally with GPUs
- Example: 128 (1 GPU) â†’ 512 (4 GPUs)

### 2. Learning Rate Adjustment
- Consider increasing learning rate slightly
- More samples per update allows higher LR

### 3. Checkpoint Frequency
- Save less frequently with multi-GPU
- Episodes complete faster

### 4. Memory Management
- Each GPU needs full model memory
- Monitor VRAM usage with nvidia-smi

## Troubleshooting

### NCCL Errors
```bash
# Set environment variables
export NCCL_DEBUG=INFO
export NCCL_TIMEOUT=3600
```

### Port Conflicts
```bash
# Use different master port
python train_ppo_lstm.py --distributed --master-port 12356
```

### GPU Selection
```bash
# Use specific GPUs
export CUDA_VISIBLE_DEVICES=0,1,2,3
python train_ppo_lstm.py --distributed --num-gpus 4
```

## Advanced Configuration

### Custom Distributed Script
The `train_ppo_lstm_distributed.py` script can be run directly:

```bash
python train_ppo_lstm_distributed.py \
  --episodes 10000 \
  --num-gpus 8 \
  --checkpoint-dir checkpoints/experiment1 \
  --master-port 12355
```

### Environment Variables
```bash
# Optimize NCCL performance
export NCCL_SOCKET_IFNAME=eth0
export NCCL_IB_DISABLE=1  # If no InfiniBand
export NCCL_P2P_DISABLE=1  # If P2P issues
```

## Performance Tips

1. **Use NVLink**: If available, enables faster GPU communication
2. **Fast Storage**: Use SSD for checkpoints and data
3. **CPU Cores**: Ensure enough CPU cores (2-4 per GPU)
4. **Network**: Use fast interconnect for multi-node training

## Comparison

| Feature | Single GPU | Multi-GPU |
|---------|------------|-----------|
| Episodes/hour | ~100 | ~350-700 |
| Update frequency | Every 128 steps | Every 128 total steps |
| Sample diversity | Single env | Multiple envs |
| Memory usage | 1x | Nx (N GPUs) |
| Checkpoint size | Standard | Same |

## Future Enhancements

- Multi-node distributed training
- Gradient accumulation options
- Mixed precision training (FP16)
- Apex/DeepSpeed integration