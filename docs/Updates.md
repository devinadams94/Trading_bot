# Training System Updates

This document tracks all improvements made to the trading bot training system.

---

## 2025-12-05: Major Training Enhancements

### 1. Separate Actor/Critic Optimizers

**Before:** Single optimizer with shared learning rate for all parameters.

**After:** Separate optimizers with different learning rates:
- Actor optimizer: 3e-4 (encoder + GRU + policy head)
- Critic optimizer: 9e-4 (3x faster for value head)

**Why it's better:**
- The critic (value function) often needs to learn faster than the actor to provide accurate baselines
- Separate optimizers prevent the actor from being held back by the critic's learning needs
- Improves explained variance from ~0.15 to ~0.5+ (target: > 0.5)

---

### 2. Extra Critic Training Epochs

**Before:** Critic trained jointly with actor for 4 PPO epochs only.

**After:** 4 additional critic-only epochs after joint PPO training (configurable via `--critic-epochs`).

**Why it's better:**
- Critic sees 2x more training updates than actor
- Value function accuracy directly impacts advantage estimation quality
- Higher explained variance = more stable policy updates

---

### 3. Enhanced Network Architecture

**Before (110K params):**
```
Encoder: Linear(64â†’128) + ReLU + LayerNorm
GRU: 1 layer, hidden=128
Policy Head: Linear(128â†’16)
Value Head: Linear(128â†’1)
```

**After (941K params with hidden_dim=256):**
```
Encoder: 2 layers with dropout
  â”œâ”€â”€ Linear(64â†’256) + ReLU + LayerNorm
  â”œâ”€â”€ Linear(256â†’256) + ReLU + LayerNorm
  â””â”€â”€ Dropout(0.1)
GRU: 2 layers, hidden=256, dropout=0.1
Policy Head: Linear(256â†’128) + ReLU + LayerNorm + Linear(128â†’16)
Value Head: Linear(256â†’128) + ReLU + LayerNorm + Linear(128â†’1)
```

**Why it's better:**
- 8.5x more parameters for better feature extraction
- 2-layer GRU captures longer temporal dependencies
- Separate hidden layers for policy/value reduce interference
- Dropout prevents overfitting
- Configurable via `--hidden-dim`, `--n-gru-layers`, `--dropout`

---

### 4. High VRAM Optimization (50GB+)

**Before:** Conservative defaults (8,192 envs, 8,192 batch size).

**After:** Aggressive defaults for high VRAM systems:
- `n_envs`: 65,536 (8x more parallel environments)
- `batch_size`: 65,536 (matches n_envs)
- `n_steps`: 512 (longer rollouts)

**Why it's better:**
- More parallel environments = more samples per second
- Larger batches = better GPU utilization
- Longer rollouts = better advantage estimation
- Estimated speedup: 8-10x faster training

**New CLI options:**
- `--high-vram`: Use aggressive settings for 50GB+ VRAM
- `--low-vram`: Use conservative settings for 8GB VRAM

---

### 5. GPU Memory Display Fix

**Before:** `torch.cuda.memory_allocated()` showed 0.00 GB (only active tensors).

**After:** `torch.cuda.memory_reserved()` shows actual GPU memory held by PyTorch.

**Why it's better:**
- Accurate representation of GPU memory usage
- Helps users understand actual VRAM consumption

---

### 6. Checkpoint Resume Functionality

**Before:** No resume capability; training always started fresh.

**After:** Automatic checkpoint resume with `--resume` flag:
- Finds latest checkpoint by modification time
- Loads model weights and optimizer states
- Handles architecture mismatches gracefully (starts fresh with warning)
- Restores metrics (best_reward, total_trades, win_rate)

**Why it's better:**
- Training can be interrupted and resumed without losing progress
- Supports both legacy single-optimizer and new dual-optimizer checkpoint formats

---

### 7. Composite Model Score

**Before:** Only trade-based metrics (win rate, avg return).

**After:** Comprehensive model score combining multiple factors:

| Component | Weight | What it Measures |
|-----------|--------|------------------|
| Profitability | 40% | avg_return Ã— win_rate |
| Reward Signal | 20% | Normalized avg_reward |
| Value Accuracy | 20% | Explained variance |
| Exploration | 10% | Entropy ratio |
| Stability | -10% | Penalty for high KL/clip |

**Why it's better:**
- Single metric to track overall model quality
- Balances trading performance with RL health metrics
- Color-coded display (green > +20, red < 0)

---

### 8. Expanded Symbol Support

**Before:** 4 assets (SPY, QQQ, IWM, CASH) with 16 portfolio regimes.

**After:** 13 assets with 24 portfolio regimes:
- ETFs: SPY, QQQ, IWM
- Tech stocks: AAPL, MSFT, NVDA, TSLA, AMZN, META, GOOGL, AMD, NFLX
- CASH

**Why it's better:**
- More diversification options
- Better exposure to high-volatility tech stocks
- Dynamic portfolio regime generation

**Usage:** `--expanded --cache-path data/v2_expanded/gpu_cache_expanded.pt`

---

## CLI Quick Reference

```bash
# Standard training with all improvements
python src/apps/train.py \
  --config configs/rl_v2_multi_asset.yaml \
  --hidden-dim 256 \
  --n-gru-layers 2 \
  --critic-epochs 4 \
  --high-vram \
  --resume

# Maximum capacity training
python src/apps/train.py \
  --hidden-dim 512 \
  --n-gru-layers 3 \
  --n-envs 131072 \
  --batch-size 131072 \
  --critic-epochs 8 \
  --expanded \
  --cache-path data/v2_expanded/gpu_cache_expanded.pt
```

---

---

### 9. VRAM Capping and Auto-Calculation

**Before:** Fixed n_envs/batch_size could cause OOM errors (tried to allocate 312,500 GB!).

**After:** Automatic VRAM-aware settings:
- `--max-vram-gpu0 45`: Cap GPU 0 at 45GB (default)
- `--max-vram-gpu1 20`: Cap GPU 1+ at 20GB (default)
- Auto-calculates safe n_envs, n_steps, batch_size based on actual VRAM
- Warns and caps if settings exceed safe limits

**Memory calculation formula:**
```
bytes_per_sample = obs_dim*4 + 8 + 4 + 4 + 4 + 1 + n_actions*4
                 â‰ˆ 341 bytes for obs_dim=64, n_actions=16

max_samples = (usable_vram_gb * 1e9) / (bytes_per_sample * 1.5 overhead)
n_envs = max_samples // n_steps
```

**Why it's better:**
- Prevents OOM crashes
- Automatically optimizes for available hardware
- Supports multi-GPU with per-GPU caps

**Conservative defaults:**
```
n_envs: 16384 (down from 65536)
n_steps: 256 (down from 512)
batch_size: 8192 (down from 65536)
```

---

---

### 10. Flexible Training Duration (Episodes/Iterations)

**Before:** Only `--timesteps` to specify training duration (default: 10M).

**After:** Three ways to specify how long to train:

| Flag | Description | Conversion |
|------|-------------|------------|
| `--timesteps 1000000` | Direct timestep count | N/A |
| `--episodes 100` | Number of episodes | episodes Ã— episode_length |
| `--iterations 5` | Number of PPO iterations | iterations Ã— n_envs Ã— n_steps |

**Examples:**
```bash
# Run for exactly 100 episodes (25,600 timesteps with default episode_length=256)
python src/apps/train.py --episodes 100

# Run for 5 training iterations (2.6M timesteps with n_envs=2048, n_steps=256)
python src/apps/train.py --iterations 5

# Traditional timestep-based (10M steps)
python src/apps/train.py --timesteps 10000000
```

**Why it's better:**
- More intuitive for quick tests (`--episodes 10`)
- Easier to reason about training duration
- Priority order: `--timesteps` > `--episodes` > `--iterations` > config default

---

### 10. Progress Indicators and Verbosity Control

**Before:** Training appeared to "stall" during PPO updates with no visible progress. Debug logs always printed.

**After:** Three verbosity modes with real-time progress indicators:

| Mode | Flag | Behavior |
|------|------|----------|
| Default | (none) | Progress bars for rollout, PPO, and critic updates |
| Verbose | `--verbose` or `-v` | Progress + debug logs (rollout stats, indexing checks, advantage stats) |
| Quiet | `--quiet` or `-q` | No progress bars, minimal output (production mode) |

**Progress indicators show:**
```
============================================================
  ðŸ”„ Iteration 1 | Steps: 0/1,048,576 (0.0%)
============================================================
  ðŸŽ² Collecting rollout (256 steps Ã— 2,048 envs)... done (2.2s)
  ðŸ“ˆ PPO Update: 500/2048 batches (24%) | Epoch 2/4
  ðŸŽ¯ Critic Update: 1500/2048 batches (73%) | Extra Epoch 2/4
```

**Why it's better:**
- User knows training is progressing during long PPO updates
- Debug output controlled via `--verbose` flag (not cluttering normal runs)
- `--quiet` mode for production/background runs
- Rollout timing visible for performance tuning

---

---

### 11. Mixed Precision Training (AMP)

**Before:** All computations in FP32.

**After:** Automatic Mixed Precision with `--amp` flag:
- Forward/backward passes use FP16 (half precision)
- Accumulation and optimizer steps use FP32 (full precision)
- GradScaler prevents underflow in FP16 gradients

**Why it's better:**
- 1.5-2x speedup on modern GPUs (RTX 4090, 6000 Ada have excellent FP16 tensor cores)
- Reduced memory bandwidth requirements
- No accuracy loss due to FP32 accumulation

**Usage:**
```bash
python src/apps/train.py --amp --high-vram --multi-gpu
```

---

### 12. Factor-Based Portfolio Regimes (Scalable Action Space)

**Before:** 16+ discrete portfolio regimes with hard-coded stock allocations. Didn't scale.

**After:** 12 factor-based regimes that scale to any stock universe:

| Idx | Name | Type | Description |
|-----|------|------|-------------|
| 0 | HOLD | hold | Keep current allocation |
| 1 | FULL_CASH | cash | 100% cash |
| 2 | DEFENSIVE | cash | 50% cash + 50% equal weight |
| 3 | LIGHT_CASH | cash | 25% cash + 75% equal weight |
| 4 | EQUAL_WEIGHT | factor | Equal weight all stocks |
| 5 | MOMENTUM | factor | Overweight top 50% by 20-day momentum |
| 6 | LOW_VOL | factor | Overweight top 50% by lowest volatility |
| 7 | QUALITY | factor | Overweight top 50% by Sharpe ratio |
| 8 | SIZE_SMALL | factor | Overweight smaller stocks |
| 9 | SAFE_MOMENTUM | mixed | 30% cash + 70% momentum tilt |
| 10 | SAFE_QUALITY | mixed | 30% cash + 70% quality tilt |
| 11 | RISK_ON | mixed | 0% cash + aggressive momentum (top 30%) |

**Factor scores computed dynamically:**
- `momentum`: 20-day price change
- `low_vol`: Inverse of 20-day volatility
- `quality`: 20-day Sharpe ratio
- `size`: Inverse of price (smaller stocks score higher)

**Why it's better:**
- Same 12 actions work for 3 stocks or 300 stocks
- Agent learns **WHEN** to apply factors, not **WHICH stocks** to pick
- Faster learning (simpler action space)
- Factor weights adjust dynamically based on current market conditions

---

### 13. Environment Realism Improvements

**Before:** Instant execution at close price with only transaction costs.

**After:** Four realistic trading friction models:

#### 13a. Slippage (0.05% max)
```python
slippage_pct = random(0, 0.0005)
# Buying gets worse price (higher), selling gets worse price (lower)
executed_price = close * (1 + slippage_pct)  # for buys
executed_price = close * (1 - slippage_pct)  # for sells
```

#### 13b. Bid-Ask Spread (0.02%)
```python
spread_cost = 0.0002 * abs(delta_shares) * price
```

#### 13c. T-1 Prices for Observations
```python
# Agent sees yesterday's data, executes at today's prices
obs_day_idx = day_idx - 1  # Observations from T-1
execution_day_idx = day_idx  # Execution at T
```

#### 13d. Market Impact (sqrt model)
```python
relative_size = trade_value / portfolio_value
impact_pct = 0.01 * sqrt(relative_size)
impact_cost = impact_pct * trade_value
```

**Typical trading costs by regime:**
| Regime | Cost |
|--------|------|
| HOLD | 0.00% |
| FULL_CASH | 0.00% |
| DEFENSIVE | ~0.06% |
| LIGHT_CASH | ~0.56% |
| EQUAL_WEIGHT | ~0.98% |
| Factor strategies | 0.3-1.0% |

**Why it's better:**
- Model learns to avoid excessive turnover
- Rewards are more realistic for paper/live trading
- Prevents unrealistic strategies that only work with zero friction

---

### 14. Generalized Advantage Estimation (GAE)

**Before:** Pure Monte Carlo returns (high variance).
```python
# Simple backward pass - high variance
returns[t] = reward[t] + gamma * returns[t+1] * not_done
advantages = returns - values
```

**After:** GAE with Î»=0.95 (lower variance, same bias).
```python
# TD error
delta = reward[t] + gamma * V(s_{t+1}) * not_done - V(s_t)
# GAE with temporal smoothing
gae = delta + gamma * lambda * not_done * gae
advantages = gae
returns = advantages + values
```

**GAE lambda controls bias-variance tradeoff:**
| Î» Value | Behavior |
|---------|----------|
| Î»=1.0 | Pure MC (high variance, zero bias) |
| Î»=0.0 | TD(0) (low variance, high bias) |
| Î»=0.95 | Good balance (recommended) |

**Why it's better:**
- Lower variance advantage estimates
- More stable policy updates
- Configurable via `--gae-lambda 0.95`

---

### 15. Graceful Shutdown (Ctrl+C)

**Before:** Ctrl+C killed training immediately without saving.

**After:** Graceful shutdown with checkpoint save:
1. First Ctrl+C: Sets flag, prints message, finishes current iteration
2. Saves checkpoint as `model_interrupted_YYYYMMDD_HHMMSS.pt`
3. Second Ctrl+C: Force quit (no save)

**Checkpoint includes:**
- Model weights
- Actor optimizer state
- Critic optimizer state
- Total steps completed
- Training metrics

**Resume interrupted training:**
```bash
# Auto-find latest checkpoint
python src/apps/train.py --resume

# Or specify exact checkpoint
python src/apps/train.py --checkpoint checkpoints/clstm_full/model_interrupted_20251205_123456.pt
```

**Why it's better:**
- Never lose training progress
- Safe to stop and resume anytime
- Supports long training runs with interruptions

---

### 16. Fixed DataParallel Compatibility

**Before:** `torch.inference_mode()` during rollout caused crash with multi-GPU DataParallel.
```
RuntimeError: Inference tensors cannot be saved for backward.
```

**After:** Use `torch.no_grad()` instead of `torch.inference_mode()`.

**Why:**
- `inference_mode()` creates "inference tensors" that can't be used in autograd
- `no_grad()` disables gradient tracking but tensors can still be used as inputs
- Both are fast, but `no_grad()` is compatible with DataParallel

---

## CLI Quick Reference (Updated)

```bash
# Full training with all optimizations
python src/apps/train.py \
  --config configs/rl_v2_multi_asset.yaml \
  --high-vram \
  --multi-gpu \
  --amp \
  --hidden-dim 256 \
  --n-gru-layers 2 \
  --critic-epochs 4 \
  --gae-lambda 0.95 \
  --episodes 1000

# Resume interrupted training
python src/apps/train.py --resume --high-vram --multi-gpu --amp

# Quick test (10 episodes)
python src/apps/train.py --episodes 10 --verbose
```

---

## Future Updates

<!-- Add new updates below this line -->

