"""
High-Performance Parallel Training System
Optimized for: 24 vCPUs, 141GB VRAM, 240GB RAM

Key optimizations:
1. Vectorized environments (16-24 parallel envs)
2. Batched GPU inference
3. Async data loading with prefetching
4. Multiprocessing for CPU-bound ops
5. Running reward normalization for stable training
"""

import torch
import torch.multiprocessing as mp
import numpy as np
from typing import Dict, List, Optional, Tuple
import logging
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from queue import Queue
import threading
import time
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# Set start method for multiprocessing
try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass  # Already set


class RunningMeanStd:
    """Running mean and std for reward normalization (Welford's algorithm)"""
    def __init__(self, epsilon: float = 1e-4, shape: Tuple = ()):
        self.mean = np.zeros(shape, dtype=np.float64)
        self.var = np.ones(shape, dtype=np.float64)
        self.count = epsilon

    def update(self, x: np.ndarray):
        """Update running stats with a batch of values"""
        batch_mean = np.mean(x)
        batch_var = np.var(x)
        batch_count = x.size
        self._update_from_moments(batch_mean, batch_var, batch_count)

    def _update_from_moments(self, batch_mean, batch_var, batch_count):
        delta = batch_mean - self.mean
        tot_count = self.count + batch_count

        new_mean = self.mean + delta * batch_count / tot_count
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count
        new_var = M2 / tot_count

        self.mean = new_mean
        self.var = new_var
        self.count = tot_count

    def normalize(self, x: np.ndarray, clip: float = 10.0) -> np.ndarray:
        """Normalize values using running mean/std"""
        normalized = (x - self.mean) / (np.sqrt(self.var) + 1e-8)
        return np.clip(normalized, -clip, clip)

    @property
    def std(self):
        return np.sqrt(self.var + 1e-8)


@dataclass
class RolloutBatch:
    """Batch of rollout data from multiple environments"""
    observations: Dict[str, np.ndarray]  # Shape: (n_envs, ...)
    actions: np.ndarray  # Shape: (n_envs,)
    rewards: np.ndarray  # Shape: (n_envs,)
    values: np.ndarray  # Shape: (n_envs,)
    log_probs: np.ndarray  # Shape: (n_envs,)
    dones: np.ndarray  # Shape: (n_envs,)


class VectorizedEnvWrapper:
    """Wraps multiple environments for parallel execution"""
    
    def __init__(self, env_fn, n_envs: int = 16, use_multiprocessing: bool = True):
        self.n_envs = n_envs
        self.use_multiprocessing = use_multiprocessing
        
        logger.info(f"ðŸš€ Creating {n_envs} parallel environments...")
        
        if use_multiprocessing:
            # Use process pool for true parallelism
            self.envs = []
            for i in range(n_envs):
                env = env_fn()
                self.envs.append(env)
            self.executor = ThreadPoolExecutor(max_workers=n_envs)
        else:
            # Simple list of envs (for debugging)
            self.envs = [env_fn() for _ in range(n_envs)]
            self.executor = None
        
        # Get spaces from first env
        self.observation_space = self.envs[0].observation_space
        self.action_space = self.envs[0].action_space
        self.episode_length = self.envs[0].episode_length
        
        logger.info(f"âœ… {n_envs} environments created successfully")
    
    def reset(self) -> List[Dict[str, np.ndarray]]:
        """Reset all environments in parallel"""
        if self.executor:
            futures = [self.executor.submit(env.reset) for env in self.envs]
            observations = [f.result() for f in futures]
        else:
            observations = [env.reset() for env in self.envs]
        return observations
    
    def step(self, actions: np.ndarray) -> Tuple[List[Dict], np.ndarray, np.ndarray, List[Dict]]:
        """Step all environments in parallel"""
        if self.executor:
            futures = [
                self.executor.submit(env.step, int(actions[i])) 
                for i, env in enumerate(self.envs)
            ]
            results = [f.result() for f in futures]
        else:
            results = [env.step(int(actions[i])) for i, env in enumerate(self.envs)]
        
        observations = [r[0] for r in results]
        rewards = np.array([r[1] for r in results], dtype=np.float32)
        dones = np.array([r[2] for r in results], dtype=bool)
        infos = [r[3] for r in results]
        
        return observations, rewards, dones, infos
    
    def close(self):
        """Clean up resources"""
        if self.executor:
            self.executor.shutdown(wait=False)
        for env in self.envs:
            if hasattr(env, 'close'):
                env.close()


class BatchedAgent:
    """Agent wrapper for batched inference on GPU"""
    
    def __init__(self, agent, device: torch.device, n_envs: int):
        self.agent = agent
        self.device = device
        self.n_envs = n_envs
        self.network = agent.network
    
    def batch_act(
        self, 
        observations: List[Dict[str, np.ndarray]], 
        deterministic: bool = False
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Get actions for a batch of observations
        Returns: actions, log_probs, values
        """
        # Stack observations into batched tensors
        batch_obs = self._stack_observations(observations)
        
        with torch.no_grad():
            # Forward pass on GPU (batched)
            action_logits, values = self.network.forward(batch_obs)
            
            if deterministic:
                actions = action_logits.argmax(dim=-1)
                log_probs = torch.zeros(self.n_envs, device=self.device)
            else:
                dist = torch.distributions.Categorical(logits=action_logits)
                actions = dist.sample()
                log_probs = dist.log_prob(actions)
        
        return (
            actions.cpu().numpy(),
            log_probs.cpu().numpy(),
            values.squeeze(-1).cpu().numpy()
        )
    
    def _stack_observations(self, observations: List[Dict]) -> Dict[str, torch.Tensor]:
        """Stack list of observation dicts into batched tensors"""
        keys = observations[0].keys()
        batch = {}
        for key in keys:
            stacked = np.stack([obs[key] for obs in observations], axis=0)
            batch[key] = torch.tensor(stacked, dtype=torch.float32, device=self.device)
        return batch


class ParallelRolloutCollector:
    """Collects rollouts from vectorized environments efficiently"""

    def __init__(
        self,
        vec_env: VectorizedEnvWrapper,
        batched_agent: BatchedAgent,
        n_steps: int = 200,
        gamma: float = 0.99,
        gae_lambda: float = 0.95
    ):
        self.vec_env = vec_env
        self.agent = batched_agent
        self.n_steps = n_steps
        self.n_envs = vec_env.n_envs
        self.gamma = gamma
        self.gae_lambda = gae_lambda

    def collect_rollouts(self) -> Dict[str, np.ndarray]:
        """
        Collect rollouts from all environments in parallel
        Returns batched experience data ready for training
        """
        n_envs = self.n_envs
        n_steps = self.n_steps

        # Pre-allocate arrays for efficiency
        all_obs = {key: [] for key in self.vec_env.observation_space.spaces.keys()}
        all_actions = np.zeros((n_steps, n_envs), dtype=np.int64)
        all_rewards = np.zeros((n_steps, n_envs), dtype=np.float32)
        all_values = np.zeros((n_steps, n_envs), dtype=np.float32)
        all_log_probs = np.zeros((n_steps, n_envs), dtype=np.float32)
        all_dones = np.zeros((n_steps, n_envs), dtype=bool)

        # Reset environments
        observations = self.vec_env.reset()

        episode_returns = [0.0] * n_envs
        episode_lengths = [0] * n_envs
        completed_episodes = []

        start_time = time.time()

        for step in range(n_steps):
            # Batch inference
            actions, log_probs, values = self.agent.batch_act(observations)

            # Store transition
            for key in all_obs:
                all_obs[key].append(np.stack([obs[key] for obs in observations]))
            all_actions[step] = actions
            all_values[step] = values
            all_log_probs[step] = log_probs

            # Step all envs in parallel
            next_observations, rewards, dones, infos = self.vec_env.step(actions)

            all_rewards[step] = rewards
            all_dones[step] = dones

            # Track episode stats
            for i in range(n_envs):
                episode_returns[i] += rewards[i]
                episode_lengths[i] += 1

                if dones[i]:
                    completed_episodes.append({
                        'return': episode_returns[i],
                        'length': episode_lengths[i],
                        'env_id': i
                    })
                    episode_returns[i] = 0.0
                    episode_lengths[i] = 0

            observations = next_observations

        elapsed = time.time() - start_time
        steps_per_sec = (n_steps * n_envs) / elapsed

        logger.info(f"ðŸ“Š Collected {n_steps * n_envs} steps in {elapsed:.2f}s ({steps_per_sec:.0f} steps/sec)")
        logger.info(f"   Completed {len(completed_episodes)} episodes")

        # Stack observations
        for key in all_obs:
            all_obs[key] = np.stack(all_obs[key], axis=0)  # (n_steps, n_envs, ...)

        # Compute advantages using GAE
        advantages, returns = self._compute_gae(all_rewards, all_values, all_dones)

        return {
            'observations': all_obs,
            'actions': all_actions,
            'rewards': all_rewards,
            'values': all_values,
            'log_probs': all_log_probs,
            'dones': all_dones,
            'advantages': advantages,
            'returns': returns,
            'completed_episodes': completed_episodes,
            'steps_per_sec': steps_per_sec
        }

    def _compute_gae(
        self,
        rewards: np.ndarray,
        values: np.ndarray,
        dones: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Compute Generalized Advantage Estimation"""
        n_steps, n_envs = rewards.shape
        advantages = np.zeros_like(rewards)
        last_gae = np.zeros(n_envs)

        for t in reversed(range(n_steps)):
            if t == n_steps - 1:
                next_values = np.zeros(n_envs)  # Bootstrap with 0 at end
            else:
                next_values = values[t + 1]

            delta = rewards[t] + self.gamma * next_values * (1 - dones[t]) - values[t]
            last_gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_gae
            advantages[t] = last_gae

        returns = advantages + values
        return advantages, returns


class ParallelPPOTrainer:
    """
    High-performance PPO trainer using parallel environments

    Hardware utilization:
    - 16-24 parallel environments on CPU cores
    - Batched inference on GPU
    - Efficient data transfer with pinned memory
    - Running reward/return normalization for stable training
    - Mixed precision (AMP) for 2x faster training on modern GPUs
    """

    def __init__(
        self,
        env_fn,
        agent,
        n_envs: int = 16,
        n_steps: int = 200,
        n_epochs: int = 10,
        batch_size: int = 512,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_epsilon: float = 0.2,
        value_coef: float = 0.5,
        entropy_coef: float = 0.01,
        max_grad_norm: float = 0.5,
        device: str = 'cuda',
        normalize_rewards: bool = True,
        reward_clip: float = 10.0,
        use_amp: bool = True  # Enable mixed precision by default
    ):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.n_envs = n_envs
        self.n_steps = n_steps
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.clip_epsilon = clip_epsilon
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.gamma = gamma

        # Mixed precision (AMP) for 2x faster training
        self.use_amp = use_amp and torch.cuda.is_available()
        if self.use_amp:
            # Use bfloat16 for H100/H200, float16 for older GPUs
            gpu_name = torch.cuda.get_device_name(0).lower()
            if 'h100' in gpu_name or 'h200' in gpu_name or 'a100' in gpu_name:
                self.amp_dtype = torch.bfloat16
            else:
                self.amp_dtype = torch.float16
            self.scaler = torch.amp.GradScaler('cuda', enabled=(self.amp_dtype == torch.float16))
        else:
            self.amp_dtype = torch.float32
            self.scaler = None

        # Reward normalization for stable training
        self.normalize_rewards = normalize_rewards
        self.reward_clip = reward_clip
        self.reward_normalizer = RunningMeanStd() if normalize_rewards else None
        self.return_normalizer = RunningMeanStd() if normalize_rewards else None

        # Create vectorized environments
        self.vec_env = VectorizedEnvWrapper(env_fn, n_envs=n_envs)

        # Wrap agent for batched inference
        self.agent = agent
        self.batched_agent = BatchedAgent(agent, self.device, n_envs)

        # Create rollout collector
        self.rollout_collector = ParallelRolloutCollector(
            self.vec_env, self.batched_agent, n_steps, gamma, gae_lambda
        )

        # Training stats
        self.total_steps = 0
        self.total_episodes = 0

        logger.info(f"ðŸš€ ParallelPPOTrainer initialized")
        logger.info(f"   Environments: {n_envs}")
        logger.info(f"   Steps per rollout: {n_steps}")
        logger.info(f"   Total batch size: {n_envs * n_steps}")
        logger.info(f"   Device: {self.device}")
        logger.info(f"   Mixed Precision: {self.use_amp} ({self.amp_dtype})")
        logger.info(f"   Reward normalization: {normalize_rewards}")

    def train_iteration(self) -> Dict:
        """Run one training iteration (collect + train)"""
        # Collect rollouts from all envs
        rollout_data = self.rollout_collector.collect_rollouts()

        # Flatten for training
        n_steps, n_envs = rollout_data['actions'].shape
        total_samples = n_steps * n_envs

        # Reshape to (total_samples, ...)
        flat_obs = {}
        for key, val in rollout_data['observations'].items():
            flat_obs[key] = val.reshape(total_samples, *val.shape[2:])

        flat_actions = rollout_data['actions'].reshape(-1)
        flat_log_probs = rollout_data['log_probs'].reshape(-1)
        flat_advantages = rollout_data['advantages'].reshape(-1)
        flat_returns = rollout_data['returns'].reshape(-1)
        flat_rewards = rollout_data['rewards'].reshape(-1)

        # Update reward normalizer with raw rewards
        if self.normalize_rewards and self.reward_normalizer:
            self.reward_normalizer.update(flat_rewards)
            self.return_normalizer.update(flat_returns)

        # Normalize advantages (always do this)
        flat_advantages = (flat_advantages - flat_advantages.mean()) / (flat_advantages.std() + 1e-8)

        # Normalize returns for value loss stability
        if self.normalize_rewards and self.return_normalizer:
            flat_returns = self.return_normalizer.normalize(flat_returns, clip=self.reward_clip)

        # Convert to tensors
        obs_tensor = {k: torch.tensor(v, dtype=torch.float32, device=self.device) for k, v in flat_obs.items()}
        actions_tensor = torch.tensor(flat_actions, dtype=torch.long, device=self.device)
        old_log_probs_tensor = torch.tensor(flat_log_probs, dtype=torch.float32, device=self.device)
        advantages_tensor = torch.tensor(flat_advantages, dtype=torch.float32, device=self.device)
        returns_tensor = torch.tensor(flat_returns, dtype=torch.float32, device=self.device)

        # Train for multiple epochs
        total_loss = 0
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy = 0
        n_updates = 0

        indices = np.arange(total_samples)

        for epoch in range(self.n_epochs):
            np.random.shuffle(indices)

            for start in range(0, total_samples, self.batch_size):
                end = min(start + self.batch_size, total_samples)
                batch_idx = indices[start:end]

                batch_obs = {k: v[batch_idx] for k, v in obs_tensor.items()}
                batch_actions = actions_tensor[batch_idx]
                batch_old_log_probs = old_log_probs_tensor[batch_idx]
                batch_advantages = advantages_tensor[batch_idx]
                batch_returns = returns_tensor[batch_idx]

                # Forward pass with AMP (mixed precision)
                with torch.amp.autocast('cuda', enabled=self.use_amp, dtype=self.amp_dtype):
                    action_logits, values = self.agent.network.forward(batch_obs)
                    dist = torch.distributions.Categorical(logits=action_logits)

                    new_log_probs = dist.log_prob(batch_actions)
                    ratio = torch.exp(new_log_probs - batch_old_log_probs)

                    # PPO loss
                    surr1 = ratio * batch_advantages
                    surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                    policy_loss = -torch.min(surr1, surr2).mean()

                    # Value loss
                    values_flat = values.squeeze(-1)
                    value_loss = torch.nn.functional.mse_loss(values_flat, batch_returns)

                    # Entropy
                    entropy = dist.entropy().mean()

                    # Combined loss
                    loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy

                # Backward pass with gradient scaling (for FP16 only)
                self.agent.ppo_optimizer.zero_grad()
                self.agent.clstm_optimizer.zero_grad()

                if self.scaler is not None:
                    self.scaler.scale(loss).backward()
                    self.scaler.unscale_(self.agent.ppo_optimizer)
                    self.scaler.unscale_(self.agent.clstm_optimizer)
                    torch.nn.utils.clip_grad_norm_(self.agent.network.parameters(), self.max_grad_norm)
                    self.scaler.step(self.agent.ppo_optimizer)
                    self.scaler.step(self.agent.clstm_optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.agent.network.parameters(), self.max_grad_norm)
                    self.agent.ppo_optimizer.step()
                    self.agent.clstm_optimizer.step()

                total_loss += loss.item()
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy += entropy.item()
                n_updates += 1

        # Update stats
        self.total_steps += total_samples
        completed = rollout_data['completed_episodes']
        self.total_episodes += len(completed)

        # Calculate episode stats
        if completed:
            avg_return = np.mean([ep['return'] for ep in completed])
            avg_length = np.mean([ep['length'] for ep in completed])
        else:
            avg_return = 0
            avg_length = 0

        return {
            'loss': total_loss / n_updates,
            'policy_loss': total_policy_loss / n_updates,
            'value_loss': total_value_loss / n_updates,
            'entropy': total_entropy / n_updates,
            'total_steps': self.total_steps,
            'total_episodes': self.total_episodes,
            'episodes_this_iter': len(completed),
            'avg_return': avg_return,
            'avg_length': avg_length,
            'steps_per_sec': rollout_data['steps_per_sec']
        }

    def train(self, total_timesteps: int, log_interval: int = 1) -> List[Dict]:
        """Train for specified number of timesteps"""
        logger.info(f"ðŸŽ¯ Training for {total_timesteps:,} timesteps")

        iteration = 0
        history = []
        start_time = time.time()

        while self.total_steps < total_timesteps:
            metrics = self.train_iteration()
            history.append(metrics)
            iteration += 1

            if iteration % log_interval == 0:
                elapsed = time.time() - start_time
                logger.info(
                    f"ðŸ“ˆ Iter {iteration} | "
                    f"Steps: {self.total_steps:,}/{total_timesteps:,} | "
                    f"Episodes: {self.total_episodes} | "
                    f"Avg Return: {metrics['avg_return']:.2f} | "
                    f"Loss: {metrics['loss']:.4f} | "
                    f"Speed: {metrics['steps_per_sec']:.0f} steps/s"
                )

        total_time = time.time() - start_time
        logger.info(f"âœ… Training complete in {total_time:.1f}s")
        logger.info(f"   Total steps: {self.total_steps:,}")
        logger.info(f"   Total episodes: {self.total_episodes}")
        logger.info(f"   Avg speed: {self.total_steps / total_time:.0f} steps/s")

        return history

    def close(self):
        """Clean up resources"""
        self.vec_env.close()

